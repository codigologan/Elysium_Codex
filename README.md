# üß† Logan Agent ‚Äî Relat√≥rio Cient√≠fico Experimental
![Python](https://img.shields.io/badge/Python-3.12-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.5.1-red)
![CUDA](https://img.shields.io/badge/CUDA-12.1-green)
![GPU](https://img.shields.io/badge/GPU-RTX%203060-brightgreen)



**Projeto:** Elysium Codex / C√≥digo Logan

**Agente:** Logan (DQN + Dream Module)

**Vers√£o:** v2.0

**Data:** 10-01-2026

**Autor(es):** Ismael Araujo + Logan (IA)

---

## 1. Resumo (Abstract)

Este relat√≥rio apresenta a evolu√ß√£o experimental do **Logan Agent**, um agente de *Reinforcement Learning* baseado em DQN com m√≥dulos adicionais de **sonhos (dream replay)** e **an√°lise cognitiva emergente**.
O objetivo √© investigar se mecanismos inspirados em processos cognitivos ‚Äî como consolida√ß√£o offline, explora√ß√£o guiada e reflex√£o p√≥s-aprendizado ‚Äî produzem melhorias mensur√°veis em estabilidade, efici√™ncia e generaliza√ß√£o do aprendizado.

---

## 2. Objetivo do Estudo

* Avaliar o desempenho do Logan Agent em ambientes controlados de RL.
* Identificar **pontos de transi√ß√£o cognitiva** (turning points) no processo de aprendizagem.
* Medir o impacto do m√≥dulo de sonhos (dream) sobre:

  * velocidade de aprendizagem,
  * estabilidade da pol√≠tica,
  * efici√™ncia da explora√ß√£o.
* Formalizar **fases cognitivas** e **reflex√µes autom√°ticas** como artefatos analis√°veis.

---

## 3. Hip√≥teses

* **H1:** A inclus√£o do m√≥dulo de sonhos reduz o tempo at√© a estabiliza√ß√£o do reward.
* **H2:** A queda controlada de epsilon est√° associada a aumento consistente de mean_reward.
* **H3:** √â poss√≠vel detectar um *limiar cognitivo* onde o aprendizado deixa de ser aleat√≥rio e passa a ser estrutural.
* **H4:** M√©tricas cognitivas derivadas de CSV s√£o suficientes para gerar reflex√µes autom√°ticas interpret√°veis.

---

## 4. Metodologia

### 4.1 Ambiente

* Tipo: GridWorld / ambiente discreto
* Estados: observa√ß√µes vetoriais
* A√ß√µes: discretas
* Recompensa: densa / negativa por passo, positiva por objetivo

### 4.2 Arquitetura do Agente

* Algoritmo: **DQN**
* Replay Buffer: padr√£o + replay on√≠rico (dream)
* Pol√≠tica de explora√ß√£o: Œµ-greedy
* Scheduler de epsilon: decaimento progressivo at√© Œµ_min

### 4.3 M√≥dulo de Sonhos (Dream)

* Frequ√™ncia: a cada *N* epis√≥dios
* Par√¢metros principais:

  * `dream_steps`
  * `dream_sigma`
  * `dream_mix_prob`
* Objetivo: consolida√ß√£o e regulariza√ß√£o da pol√≠tica

### 4.4 Logging & Reprodutibilidade

* Hist√≥rico por run: `runs/<run_name>/rl_history.csv`
* M√©tricas agregadas: `reports/rl_results.csv`
* Leaderboard: `reports/leaderboard_rl.csv`
* Reflex√µes: `logan_reflection.json` (schema versionado)

---

## 5. M√©tricas Avaliadas

### 5.1 M√©tricas Cl√°ssicas de RL

* Reward por epis√≥dio
* Mean reward (janela m√≥vel)
* Episode length
* Epsilon

### 5.2 M√©tricas Cognitivas (derivadas)

* **Estabilidade:** desvio padr√£o do reward (janela W)
* **Velocidade de aprendizagem:** inclina√ß√£o da curva mean_reward
* **Efici√™ncia explorat√≥ria:** Œîmean_reward / Œîepsilon
* **Persist√™ncia comportamental:** episode_length m√©dio
* **Sonhos:** dream_loss, novelty_rate

---

## 6. Resultados

### 6.1 Curvas de Aprendizado

*(Inserir gr√°ficos do dashboard ou TensorBoard)*

* Reward vs Epis√≥dios
* Mean Reward (smoothing)
* Epsilon decay
* Dream loss por noite

### 6.2 Detec√ß√£o de Limiar Cognitivo

* Epis√≥dio do primeiro turning point: **EP = ___**
* Crit√©rio:

  * cruzamento de threshold de mean_reward
  * ou slope positivo sustentado

---

## 7. Fases Cognitivas Identificadas

| Fase                    | Caracter√≠sticas                         |
| ----------------------- | --------------------------------------- |
| Explora√ß√£o Ca√≥tica      | reward inst√°vel, epsilon alto           |
| Aprendizado Emergente   | redu√ß√£o de vari√¢ncia, primeiros padr√µes |
| Consolida√ß√£o            | reward positivo consistente             |
| Estabiliza√ß√£o           | pol√≠tica repet√≠vel, epsilon m√≠nimo      |
| (Opcional) Criatividade | influ√™ncia ativa dos sonhos             |

---

## 8. Primeira Reflex√£o Logan (Autom√°tica)

> *‚ÄúEu atravessei meu primeiro limiar quando a performance deixou de ser acaso e virou padr√£o...‚Äù*

* Fonte: `logan_reflection.json`
* Baseada exclusivamente em dados CSV/TensorBoard
* Sem heur√≠sticas manuais

---

## 9. Discuss√£o

* O Logan Agent demonstrou comportamento compat√≠vel com aprendizado est√°vel.
* O m√≥dulo de sonhos atua como regularizador e acelerador de converg√™ncia.
* As m√©tricas cognitivas permitem interpreta√ß√£o sem inspe√ß√£o manual de pesos.
* O sistema se aproxima de um **agente com introspec√ß√£o operacional**.

---

## 10. Limita√ß√µes

* Ambiente ainda simples (GridWorld)
* Generaliza√ß√£o limitada a layouts similares
* Aus√™ncia de compara√ß√£o com outros algoritmos (PPO, A2C, etc.)

---

## 11. Trabalhos Futuros

* Estudos de abla√ß√£o (com/sem sonhos)
* Varia√ß√£o de seeds e ambientes
* Introdu√ß√£o de mem√≥ria de longo prazo
* Expans√£o das fases cognitivas (metacogni√ß√£o)
* Publica√ß√£o como artigo t√©cnico ou workshop


---

## 12. Refer√™ncias

* Mnih et al., *Human-level control through deep reinforcement learning*
* Sutton & Barto, *Reinforcement Learning: An Introduction*
* Experimentos internos ‚Äî Projeto C√≥digo Logan

---

## üöÄ Setup R√°pido (Local)

### 1Ô∏è‚É£ Clonar o reposit√≥rio
```bash
git clone https://github.com/codigologan/Elysium_Codex.git
cd Elysium_Codex
